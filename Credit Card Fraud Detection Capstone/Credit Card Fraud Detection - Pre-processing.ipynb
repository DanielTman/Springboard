{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import necessary packages\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load the data\ndf = pd.read_csv('../input/creditcard-noduplicates/creditcard_noduplicates.csv')\ndf.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the explanatory variables\nX = df.drop(['Class'], axis=1)\n\n# Declare target variable\ny = df['Class']\n\n# I call StandardScaler to scale all the variables but the variables most important to scale are Time and Amount\n# I also fit the scaler to the data and then transform it\nX_scaled = StandardScaler().fit_transform(X)\n\n# check to see data has been scaled\nX_scaled","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"array([[-1.99682292, -0.70108232, -0.04168726, ...,  0.33303251,\n        -0.06584955,  0.24419951],\n       [-1.99682292,  0.60879165,  0.16413764, ..., -0.0271543 ,\n         0.0432187 , -0.34258399],\n       [-1.99680186, -0.7003364 , -0.81133678, ..., -0.14432548,\n        -0.1838243 ,  1.15889967],\n       ...,\n       [ 1.64227757,  0.98235398, -0.18043304, ...,  0.00680174,\n        -0.08264021, -0.0822395 ],\n       [ 1.64227757, -0.12646526,  0.32465977, ...,  0.27052318,\n         0.31700384, -0.31339058],\n       [ 1.64236181, -0.27686005, -0.1127094 , ..., -0.01055821,\n         0.03994074,  0.51329005]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remember in prior steps we identified the 'Class' variable to be highly imbalanced\n# get the counts of fraud and non-fraud transactions\nprint(Counter(y))","execution_count":4,"outputs":[{"output_type":"stream","text":"Counter({0: 283253, 1: 473})\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Inorder to handle the highly imbalanced target variable I perform random undersampling to randomly discard instances from the majority class. One thing to note with random undersampling is that there is a risk the classification models will not perform as accurate since there is a great amount of information loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform Random UnderSampling so their will be only 473 non-fraudulent transactions\nrus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_resample(X, y)\n\nprint(Counter(y_rus))","execution_count":5,"outputs":[{"output_type":"stream","text":"Counter({0: 473, 1: 473})\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split original data and random undersampled data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\nX_rus_train, X_rus_test, y_rus_train, y_rus_test = train_test_split(X_rus, y_rus, test_size=0.20)","execution_count":6,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}